{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy.linalg as la\n",
    "np.set_printoptions(precision=3)\n",
    "def pp_float_list(ps):#pretty print functionality\n",
    "    return [\"%2.3f\" % p for p in ps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ein Unterverzeichnis oder eine Datei mit dem Namen \"CAR\" existiert bereits.\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 51867  100 51867    0     0  79474      0 --:--:-- --:--:-- --:--:-- 79672\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  3097  100  3097    0     0   8795      0 --:--:-- --:--:-- --:--:--  8823\n",
      "Der Befehl \"cat\" ist entweder falsch geschrieben oder\n",
      "konnte nicht gefunden werden.\n"
     ]
    }
   ],
   "source": [
    "#shell scripts for downloading the data and placing it in a corresponding directory\n",
    "!mkdir CAR \n",
    "!curl -o CAR/data \"http://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\"\n",
    "!curl -o CAR/description \"http://archive.ics.uci.edu/ml/machine-learning-databases/car/car.names\"\n",
    "#download the description and display it here.\n",
    "!cat CAR/description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv-file has no header, so we define it manually\n",
    "col_names = ['price_buy', 'price_main', 'n_doors', 'n_persons', 'lug_boot', 'safety', 'recommendation']\n",
    "df = pd.read_csv(\"./CAR/data\", header=None, names=col_names)\n",
    "\n",
    "# All attributes are categorical - a mix of strings and integers.\n",
    "# We simply map the categorical values of each attribute to a set of distinct integers\n",
    "ai2an_map = col_names\n",
    "ai2aiv2aivn_map = []\n",
    "enc_cols = []\n",
    "for col in df.columns:\n",
    "    df[col] = df[col].astype('category')\n",
    "    a = np.array(df[col].cat.codes.values).reshape((-1,1))\n",
    "    enc_cols.append(a)\n",
    "    ai2aiv2aivn_map.append(list(df[col].cat.categories.values))\n",
    "    \n",
    "# Get the data as numpy 2d-matrix (n_samples, n_features)\n",
    "feature_names = ai2an_map\n",
    "feature_value_names = ai2aiv2aivn_map[:6]\n",
    "class_label_names = ai2aiv2aivn_map[6]\n",
    "\n",
    "dataset = np.hstack(enc_cols)\n",
    "X, y = dataset[:,:6], dataset[:,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _entropy(p):\n",
    "    \"\"\"\n",
    "    p: class frequencies as numpy array with np.sum(p)=1\n",
    "    returns: impurity according to entropy criterion\n",
    "    \"\"\"\n",
    "    idx = np.where(p == 0.) #consider 0*log(0) as 0\n",
    "    p[idx] = 1.\n",
    "  \n",
    "    r = p * np.log2(p)\n",
    "    \n",
    "    return -np.sum(r)\n",
    "\n",
    "def _gini(p):\n",
    "    \"\"\"\n",
    "    p: class frequencies as numpy array with np.sum(p)=1\n",
    "    returns: impurity according to gini criterion\n",
    "    \"\"\"\n",
    "    return 1. - np.sum(p**2)\n",
    "\n",
    "def _misclass(p):\n",
    "    \"\"\"\n",
    "    p: class frequencies as numpy array with np.sum(p)=1\n",
    "    returns: impurity according to misclassification rate\n",
    "    \"\"\"\n",
    "    return 1-np.max(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def impurity_reduction(X, a_i, y, impurity, verbose=0):\n",
    "    \"\"\"\n",
    "    Summary: This function evaluates the impurity reduction achieved by the \n",
    "    attribute `a_i` when the attribute were used for partitioning the dataset `X`.\n",
    "    \n",
    "    X: data matrix n rows, d columns\n",
    "    a_i: column index of the attribute to evaluate the impurity reduction for\n",
    "    y: class label vector with n rows and 1 column\n",
    "    impurity: impurity function of the form impurity(p_1....p_k) with k=|X[a].unique|\n",
    "    \n",
    "    returns: impurity reduction\n",
    "    \"\"\"\n",
    "    \n",
    "    N, d = float(X.shape[0]), float(X.shape[1])\n",
    "\n",
    "    y_v = np.unique(y)\n",
    "    \n",
    "    # Compute relative frequency of each class in X\n",
    "    p = (1. / N) * np.array([np.sum(y==c) for c in y_v])\n",
    "    # ..and corresponding impurity l(D)\n",
    "    H_p = impurity(p)\n",
    "    \n",
    "    if verbose: print (\"\\t Impurity %0.3f: %s\" % (H_p, pp_float_list(p)))\n",
    "    \n",
    "    # a_v is an array of unique values in column a_i\n",
    "    a_v = np.unique(X[:, a_i])\n",
    "    \n",
    "    \"\"\"\n",
    "    Create and evaluate splitting of X induced by attribute a_i\n",
    "    We assume nominal features and perform m-ary splitting.\n",
    "    \n",
    "    We partition the dataset X into as many subsets as there are different values\n",
    "    in attribute a_i. Each subset contains the exactly those data points that have \n",
    "    the value `a_vv` in attribute `a_i`. The following steps are required:\n",
    "    - Once we have determined which data points belong to a subset (`mask_a`), \n",
    "      we can look up their corresponding class labels in the vector of \n",
    "      class labels `y`.\n",
    "    - Then we count how many times each class label appears in the subset and\n",
    "      calculate the relative frequencies `pa`.\n",
    "    - Now we can calculate the impurity of the subset `H_pa_vv` based on the \n",
    "      relative frequencies `pa` of the classes that are present in the subset.\n",
    "    - We add the impurity of the subset `H_pa_vv` to a list `H_pa`.\n",
    "    - ...and repeat the process until we have evaluated all subsets\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    H_pa = []\n",
    "    for a_vv in a_v:\n",
    "        mask_a = X[:, a_i] == a_vv\n",
    "        N_a = float(mask_a.sum())\n",
    "                     \n",
    "        # Compute relative frequency of each class in X[mask_a]\n",
    "        pa = (1. / N_a) * np.array([np.sum(y[mask_a] == c) for c in y_v])\n",
    "        H_pa_vv = (N_a / N) * impurity(pa)\n",
    "        H_pa.append(H_pa_vv)\n",
    "        if verbose: print (\"\\t\\t Impurity %0.3f for attribute %d with value %s: \" % (H_pa[-1], a_i, a_vv), pp_float_list(pa))\n",
    "    \n",
    "    \"\"\"\n",
    "    The impurity reduction (aka \"information gain\" if we use entropy as a measure),\n",
    "    is given by the difference of the impurity of the whole dataset `H_p` and the \n",
    "    weighted sum of the impurities of the subsets `np.sum(H_pa)`.\n",
    "    \"\"\"\n",
    "    IR = H_p - np.sum(H_pa)\n",
    "    if verbose:  print (\"\\t Estimated reduction %0.3f\" % IR)\n",
    "    return IR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_split_attribute(X, y, attributes, impurity, verbose=0):\n",
    "    \"\"\"\n",
    "    X: data matrix n rows, d columns\n",
    "    y: vector with n rows, 1 column containing the class labels\n",
    "    attributes: A dictionary. The key corresponds to an attribute's column index in X and the value is a list of values the attribute can take (its domain).\n",
    "    impurity: impurity function of the form impurity(p_1....p_k) with k=|y.unique|\n",
    "    returns: (1) column idx of attribute with maximum impurity reduction and (2) impurity reduction\n",
    "    \"\"\"\n",
    "    if type(X) == \"numpy.ndarray\":\n",
    "        N, d = X.shape\n",
    "    else:\n",
    "        N, d = len(X), len(X[0])\n",
    "    IR = [0.] * d\n",
    "    \n",
    "    # TODO\n",
    "    for i,key in enumerate(attributes.keys()):\n",
    "        IR[i] = impurity_reduction(X,key,y,impurity)\n",
    "    \n",
    "    return np.argmax(IR), np.max(IR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def most_common_class(y):\n",
    "    \"\"\"\n",
    "    :param y: the vector of class labels, i.e. the target\n",
    "    returns: (1) the most frequent class label in 'y' and (2) a boolean flag indicating whether y is pure\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO\n",
    "    ctemp = {}\n",
    "    for c in y:\n",
    "        if ctemp.__contains__(c):\n",
    "            ctemp[c] += 1\n",
    "        else:\n",
    "            ctemp[c] = 1\n",
    "    \n",
    "    maxi = max(ctemp, key = ctemp.get)\n",
    "\n",
    "    return maxi, ctemp[maxi] == len(y) # len(ctemp) == 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# splitting Attribute, Impurity Reduction, Children, majority Label, isPure, Size, ID\n",
      "('safety', '0.262', None, 'n_doors', 'Impure', '1210/1728', 0) - Root\n",
      "222\n"
     ]
    }
   ],
   "source": [
    "class DecisionNode(object):\n",
    "    NODEID = 0\n",
    "    \n",
    "    def __init__(self, attr=-1, impurityReduction=-1, children=None, label=None, isPure=False, X=None, y=None, isPerfectSplit=False, maxDepth=5):\n",
    "        self.attr = attr # attribute with maximum impurity reduction before split\n",
    "        self.impurityReduction = impurityReduction # the amount of impurity reduction down from the parent\n",
    "        self.children = children # list of new nodes, split according to impurity\n",
    "        self.label = label # label with the most occurences in this node\n",
    "        self.isPure = isPure # tells whether this node contains just 1 class\n",
    "        self.id = DecisionNode.NODEID # node id\n",
    "        DecisionNode.NODEID += 1 # id counter\n",
    "        self.X = np.array(X)\n",
    "        self.y = np.array(y)\n",
    "        self.isPerfectSplit = isPerfectSplit\n",
    "        self.maxDepth = maxDepth\n",
    "        \n",
    "        \n",
    "    def make_children(self):\n",
    "        self.combinedXy = np.concatenate(((self.X, self.y.reshape(-1,1))), axis=1) # X with an extra column for y\n",
    "        self.adjustedXy = np.array([[row for row in self.combinedXy if row[self.attr] == each] for each in np.unique(self.X[:,self.attr])]) # 3d np array where the data is split according to max impurity reduction\n",
    "        self.adjustedX = [np.concatenate((splitSet[:,0:self.attr], splitSet[:,self.attr+1:-1]), axis=1) for splitSet in self.adjustedXy] # list of subsets of X without the splitting column and y\n",
    "        self.adjustedy = [splitSet[:,-1] for splitSet in self.adjustedXy] # list of classes (y)\n",
    "        self.adjustedattributes = {k: None for k in range(len(self.adjustedX[0][0]))} # list of possible attributes to split\n",
    "        \n",
    "        tempChildren = []\n",
    "        for i in range(len(self.adjustedX)):\n",
    "            attribute, impurityReduction = get_split_attribute(self.adjustedX[i],self.adjustedy[i],self.adjustedattributes,_entropy)\n",
    "            label, isPure = most_common_class(self.adjustedy[i])\n",
    "            tempChildren.append(DecisionNode(attribute, impurityReduction, None, label, isPure, self.adjustedX[i], self.adjustedy[i]))\n",
    "        self.children = tempChildren\n",
    "        #[print(childe) for childe in self.children]\n",
    "        \n",
    "    def make_grandchildren(self):\n",
    "        DecisionNode.helper(self.children)\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def helper(agenda):\n",
    "        tempAgenda = []\n",
    "        for child in agenda:\n",
    "            if not child.isPure and len(Root.X[0]) - len(child.X[0]) < child.maxDepth:\n",
    "                child.make_children()\n",
    "            if child.children != None:\n",
    "                [tempAgenda.append(childe) for childe in child.children] \n",
    "        if len(tempAgenda) > 0:\n",
    "            DecisionNode.helper(tempAgenda)        \n",
    "        else:\n",
    "            return \n",
    "           \n",
    "    def __str__(self): # toString()\n",
    "        result = str((\n",
    "                    feature_names[self.attr], \n",
    "                    self.impurityReduction.__format__(\".3f\"), \n",
    "                    self.children, feature_names[self.label], \n",
    "                    \"Pure\" if self.isPure else \"Impure\", \n",
    "                    f\"{np.sum([1 for label in self.y if label == self.label])}/{len(self.X)}\", \n",
    "                    self.id\n",
    "                    ))\n",
    "        \n",
    "        if self.id == 0: result += \" - Root\"\n",
    "        elif self.id < 4: result += \" - Children\"\n",
    "        else: result += \" - Grandchildren\"\n",
    "        \n",
    "        if self.id > 0: return result\n",
    "        else: return \"# splitting Attribute, Impurity Reduction, Children, majority Label, isPure, Size, ID\\n\" + result\n",
    "\n",
    "# initial Root setup        \n",
    "attribute, impurityReduction = get_split_attribute(X, y, {k: None for k in range(X.shape[1])}, _entropy)\n",
    "label, isPure = most_common_class(y)\n",
    "Root = DecisionNode(attribute, impurityReduction, None, label, isPure, X, y)\n",
    "print(Root)\n",
    "# Root setup end\n",
    "\n",
    "Root.make_children()\n",
    "Root.make_grandchildren()\n",
    "print(DecisionNode.NODEID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Ex06\n",
    "1\n",
    "Model Stacking:\n",
    "use different learning algorithms on the data and afterwards a combiner with the results of the previous learners as inputs\n",
    "Boosting:\n",
    "train different learners sequentially and focus on the wrong results by using different weights in the latter learners\n",
    "Bagging:\n",
    "sample different sets from the data and train multiple learners on it. afterwards make a majority vote from the results\n",
    "\n",
    "2\n",
    "while bagging is an idea of a learning algorithm, random forests are an explicit instance. different trees are being trained with random subsets of attributes\n",
    "\n",
    "3a\n",
    "client is atomic and unique and can perfectly determine the risk but is just an id, from which nothing can be exrapolated or learned\n",
    "3b\n",
    "TODO?\n",
    "\n",
    "4\n",
    "the set needs to have mixed classes on x1 axis at the same x2 value\n",
    "\"\"\" "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "210px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "vscode": {
   "interpreter": {
    "hash": "e2f04cf2f7544c889a1a0098c1cf78ced2c664ea38df0d27a9b899b5831aa2a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
