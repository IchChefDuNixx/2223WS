{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a67e415e-6179-433d-b8ca-a8fda88c1f64",
   "metadata": {},
   "source": [
    "# Classifier Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2971b3f-d9dc-4db7-b437-97ce1aa08faf",
   "metadata": {},
   "source": [
    "## Task 1: Hold-Out\n",
    "\n",
    "Implement the Hold-Out method in Python. Your function should accept a list of arbitrary elements and a parameter `training_set_ratio` (e.g. 0.8).\n",
    "The function should return a training set and a testing set as specified by the `training_set_ratio`.\n",
    "\n",
    "Note: Normally, we would use existing implementations (see below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8350f59-903c-456e-995b-59f056fd6e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c35b859-b8ee-481a-8fbc-f6eef05d981d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5, 3, 9, 10, 7, 1, 2, 4], [8, 6]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1, 2, 3, 4 , 5, 6, 7, 8, 9, 10]\n",
    "train_test_split(x, train_size=.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5ecc75-9350-4f4a-bdfb-1a0a23d9dddc",
   "metadata": {},
   "source": [
    "## Task 2: $k$-fold Cross-Validation\n",
    "\n",
    "Below, you see how you can use $k$-fold CV in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "688dea3a-93b2-48c8-94fd-0d19a309223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1, 2, 3, 4 , 5, 6, 7, 8, 9, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ca46de5-216f-47be-bb0e-aed11890610b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 5 6 7 8 9] [0 1 2 3]\n",
      "[0 1 2 3 7 8 9] [4 5 6]\n",
      "[0 1 2 3 4 5 6] [7 8 9]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "splits = KFold(3, shuffle=False)\n",
    "for i, (train_index, test_index) in enumerate(splits.split(x)):\n",
    "    print(train_index, test_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428ba199-3218-4b14-80f6-a86c99a2a1e5",
   "metadata": {},
   "source": [
    "Now, implement the $k$-fold splitting for cross-validation in Python. \n",
    "\n",
    "Note: This is rather a Python exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6182c025-dc9b-4597-9528-67d61943ab35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [[4, 10, 3], [8, 1, 6, 7, 2, 9, 5]], 1: [[8, 1, 6], [4, 10, 3, 7, 2, 9, 5]], 2: [[7, 2, 9, 5], [4, 10, 3, 8, 1, 6]]}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def cv(data, k):\n",
    "    # TODO\n",
    "    result = {}\n",
    "    lenf = len(data)\n",
    "    np.random.shuffle(data)\n",
    "    for i in range(k):\n",
    "        result[i] = [data[int(((i)/k)*lenf):int(((i+1)/k)*lenf)], []]\n",
    "        result[i][1] = [x for x in data if x not in result[i][0]]\n",
    "    return result\n",
    "\n",
    "print(cv(x, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08ef60f3-d4d7-4cb0-b6a0-186f6acd3032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n1a)\\nto calculate Err*(h) you would need the real y label, which is unknown. Otherwise, why make an AI for it lol\\n1b)\\nAcc = right_guess/total\\nmiss = wrong_guess/total = 1 - right_guess/total\\n1c)\\nSince the model is trained on D, the missclassification rate is valid for D. It could be overfit and therefore valid on ONLY D\\n1d)\\n!example bank: with the missclassification rate x and the number of total instances, you can calculate an estimation for the number of missclassifications.\\nThese in turn all have a certain cost associated with it. Then the bank needs to decide, based on their internal number and costs, which model ends up being cheaper.\\n\\n2a)\\nIn the Holdout validation the input data is split into 2 sets with a certain percentage of the total data. Then the model is trained on one of those partial sets and validated on the other.\\nIn the k-fold cross validation the input data is split into k sets with about equal size. Then the model is trained k times on all but 1 of those partial sets with the set that's being left out, always being unique.\\nThe unique set is then used for validation and eventually the results of all k models are being averaged.\\n~basically having many sets and using every data point for training and testing while holdout, doesn't cross them.\\n2b)\\nwhile k-fold uses every data point for training and testing at some point, bootstrapping uses random data points of the input data multiple times, which generally results in a real subset.\\n2c)\\nyes, splitting and validation methods are independent from any performance measures.\\n\\n3a)\\nY+ = 3+7 = 10 # real plus\\nY- = 1+9 = 10 # real minus\\n3b)\\nPrediction, isReal\\nPositive, True = 3 # TP\\nPositive, False = 1 # FP\\nNegative, False = 7 # FN\\nNegative, True = 9 # TN\\n3c)\\nAcc = (3+9)/(3+7+1+9) = 12/20\\nmacro avg Precision: TP/2(TP+FP) + TN/2(TN+FN) = (3/4 + 9/16)/2 = 0.66\\nmacro avg Recall: TP/2(TP+FN) + TN/2(TN+FP) = (3/11 + 9/10)/2 = 0.59\\n4a)\\n                ^y\\n            True False\\ny   True    5    4      TP, FN\\n    False   3    5      FP, TN\\n4b)\\nPos: Prec = TP/(TP+FP) = 5/8\\n    Recall = TP/(TP+FN) = 5/9\\nNeg: Prec = TN/(TN+FN) = 5/9\\n    Recall = TN/(TN+FP) = 5/8\\n4c)\\nF1 = 2TP/(2TP+FN+FP) = 10/(10+4+3) = 10/17\\n\\n5a)\\nD = Predicted -> 1:85, 2:22, 3:13\\n5b)\\ntrivial = majority -> class 1\\nrandom? well, random 1/3 for each class\\n5c)\\nTP2 = 9\\nFP2 = 3+10\\nFN2 = 1+0\\nTN2 = 4+3+80+10 = 97\\n5d)\\nTP1 = 4\\nFP1 = 1+80\\nFN1 = 3+3\\nTN1 = 9+0+10+10 = 29\\n\\nTP3 = 10\\nFP3 = 3+0\\nFN3 = 80+10\\nTN3 = 4+3+1+9 = 17\\n\\nmicro avg Precision = (4+9+10)/(4+9+10 + 13+81+3) = 0.19\\nmicro avg Recall = (4+9+10)/(4+9+10 + 1+6+90) = 0.19\\n\\nmacro avg Precision = (4/85 + 9/22 + 10/13)/3 = 0.41\\nmacro avg Recall = (4/10 + 9/10 + 10/90)/3 = 0.47\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#10\n",
    "\"\"\"\n",
    "1a)\n",
    "to calculate Err*(h) you would need the real y label, which is unknown. Otherwise, why make an AI for it lol\n",
    "1b)\n",
    "Acc = right_guess/total\n",
    "miss = wrong_guess/total = 1 - right_guess/total\n",
    "1c)\n",
    "Since the model is trained on D, the missclassification rate is valid for D. It could be overfit and therefore valid on ONLY D\n",
    "1d)\n",
    "!example bank: with the missclassification rate x and the number of total instances, you can calculate an estimation for the number of missclassifications.\n",
    "These in turn all have a certain cost associated with it. Then the bank needs to decide, based on their internal number and costs, which model ends up being cheaper.\n",
    "\n",
    "2a)\n",
    "In the Holdout validation the input data is split into 2 sets with a certain percentage of the total data. Then the model is trained on one of those partial sets and validated on the other.\n",
    "In the k-fold cross validation the input data is split into k sets with about equal size. Then the model is trained k times on all but 1 of those partial sets with the set that's being left out, always being unique.\n",
    "The unique set is then used for validation and eventually the results of all k models are being averaged.\n",
    "~basically having many sets and using every data point for training and testing while holdout, doesn't cross them.\n",
    "2b)\n",
    "while k-fold uses every data point for training and testing at some point, bootstrapping uses random data points of the input data multiple times, which generally results in a real subset.\n",
    "2c)\n",
    "yes, splitting and validation methods are independent from any performance measures.\n",
    "\n",
    "3a)\n",
    "Y+ = 3+7 = 10 # real plus\n",
    "Y- = 1+9 = 10 # real minus\n",
    "3b)\n",
    "Prediction, isReal\n",
    "Positive, True = 3 # TP\n",
    "Positive, False = 1 # FP\n",
    "Negative, False = 7 # FN\n",
    "Negative, True = 9 # TN\n",
    "3c)\n",
    "Acc = (3+9)/(3+7+1+9) = 12/20\n",
    "macro avg Precision: TP/2(TP+FP) + TN/2(TN+FN) = (3/4 + 9/16)/2 = 0.66\n",
    "macro avg Recall: TP/2(TP+FN) + TN/2(TN+FP) = (3/11 + 9/10)/2 = 0.59\n",
    "4a)\n",
    "                ^y\n",
    "            True False\n",
    "y   True    5    4      TP, FN\n",
    "    False   3    5      FP, TN\n",
    "4b)\n",
    "Pos: Prec = TP/(TP+FP) = 5/8\n",
    "    Recall = TP/(TP+FN) = 5/9\n",
    "Neg: Prec = TN/(TN+FN) = 5/9\n",
    "    Recall = TN/(TN+FP) = 5/8\n",
    "4c)\n",
    "F1 = 2TP/(2TP+FN+FP) = 10/(10+4+3) = 10/17\n",
    "\n",
    "5a)\n",
    "D = Predicted -> 1:85, 2:22, 3:13\n",
    "5b)\n",
    "trivial = majority -> class 1\n",
    "random? well, random 1/3 for each class\n",
    "5c)\n",
    "TP2 = 9\n",
    "FP2 = 3+10\n",
    "FN2 = 1+0\n",
    "TN2 = 4+3+80+10 = 97\n",
    "5d)\n",
    "TP1 = 4\n",
    "FP1 = 1+80\n",
    "FN1 = 3+3\n",
    "TN1 = 9+0+10+10 = 29\n",
    "\n",
    "TP3 = 10\n",
    "FP3 = 3+0\n",
    "FN3 = 80+10\n",
    "TN3 = 4+3+1+9 = 17\n",
    "\n",
    "micro avg Precision = (4+9+10)/(4+9+10 + 13+81+3) = 0.19\n",
    "micro avg Recall = (4+9+10)/(4+9+10 + 1+6+90) = 0.19\n",
    "\n",
    "macro avg Precision = (4/85 + 9/22 + 10/13)/3 = 0.41\n",
    "macro avg Recall = (4/10 + 9/10 + 10/90)/3 = 0.47\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "e2f04cf2f7544c889a1a0098c1cf78ced2c664ea38df0d27a9b899b5831aa2a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
