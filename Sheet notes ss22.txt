Sheet notes ss22

URL:
Chapter 0
7-8 SV vs USV
9 Techniques
16 Properties of a similarity function
21,22 Hamming Distance, Edit Distance (note just the existence of them)
26 K-mean algorithm
31 K-mean strengths
32 rule of thumb k=sqrt(n/2)

Chapter 1
5,6 ideas for loss function
9,10 Elbow method using distortion, basic idea
12,13 Silhouette score formula, low/high meaning
24,30 Linkage
29 Understanding dendrogram (highest vertical distance)
32 Agnes vs Diana
33 K-Mean vs H-Clustering

Chapter 2
Idea: write down the method signature of certain classes (e.g. DBSCAN(X,eps=default_value,minpts=default_value) etc.)
6 DBSCAN Overviews
8,15 types of points
12 algorithm
26 epsilon elbow method
31 comparison

Chapter 3
4-8 examples for use-cases
12 Label encoding
13 One hot encoding
14,15 what to do with data
17,20,22 Scaler functions

Chapter 4
8 Curse of Dimensionality
15 Benefits of Dimensionality Reduction
22 Usage of PCA
25 projection
27 mafs
30 visualization
31 steps for pca
32 more mafs
33,34 cov
35,36 number of pcs

Chapter 5
8 PCA simplified
16 how to project on PC?
17 total variation formula
18,19 more cov (19 real eigenvalues, diagonalization)
20,21 cov properties

Chapter 6
9 Eigenface algorithm
11 PCA vs SNE
14 SNE
17-23 t-SNE algorithm

Chapter 7
Autoencoder tasks = {compress-decompress, denoise, anomaly detection}
13,14 Visualizations
keras code?

Chapter 8
4 diagram
9,11 RL characteristics
15 Markov Property
24 Discount Factor
25 Markov Decision Process
27 Qt(a)

Mock exam missing knowledge
5a layer size formulas
6b MDP
6c Q-Learning

Chapter 9


Chapter 10

Stoch:

BSOM:
part 3
(diagram of a rectangle Omega with subsets A,B + page 4 def 3)
sigma Algebra (collection of subsets in discr.) -> discrete=Powerset(Omega); abs. cont.=Borelset(Omega)
discrete=countably many outcome; abs. cont.=uncountably many outcomes; mixed=at least 1 discrete point and 1 continous part
discrete pmf = sum of single probabilities within set
abs. cont. pdf = integral of f(x) over set (f(x) = distribution like U,Exp,N)
part 5
definition of a (discrete) probability space
part 6
Hypergeometric: H(N,Kn,n)
-P({k1,k2,k3})=(N1 choose k1 * N2 choose k2 * N3 choose k3)/(N choose n)
-2 class: P({k})=((N1 choose k1) * (N-N1 choose n-k))/(N choose n)
part 9
change of integral interval with set
part 11
Bernoulli: Bin(1,p)
part 12
determine the cdf/how to take the integral
part 13
independence of random variables, joint cdf
page 12 def 13
page 13 ex 14, prop 15
part 14
formula for manually calculating the expectation
part 15
properties of the expectation
part 16
formula for manually calculating the variance
properties of the variance 
part 18 more properties of var and std
proofs

*determine rules for interchanging sums and unions or sums and products etc
/*Big table for all distributions*/



Binomial: B(n,p), n=trials,p=chance of success, E(X)=np, Var(X)=np(1-p)
-P({X=k})=(n choose k)* p^k * (1-p)^(n-k)


Multinomial: M(n,p0...pm), same as having m Binomials with n trials each but different p, P(A=x,B=y,C=z)=((x+y+z)!/(x!*y!*z!))*(pa^x)*(pb^y)*(pc^z)



Poisson: P(lambda)
Geometric: Geo(p)

Uniform: U(a,b)
Exponential: Exp(lambda)
Normal: N(mu,sigma^2)



definition of a (discrete) probability space
5 remark 4
6 prop 10 i-v
6 prop 2 i,ii
8 def 1
9 def 5
9 prop 8 i,ii



17 def 2
17 prop 4

28 prop 7




find out the max/min of multiple variables (2.1b)
log rules!
poisson limit theorem. B(n,p) -> P(n*p)
	H(N,K,n) -> B(K/N)
definition of E and Var for discrete and continous caseshttps://proofwiki.org/wiki/Variance_of_Continuous_Uniform_Distribution
integral over x^2 * f(x) dx - E(X)^2
derivation and integration rules
how to deal with |X| (eg 2 different cases or P({-a < X < a}))
diagrams of all, just regular or just weird distros
ex 6.1 min(a,b) > x <=> (a > x) ^ (b > x)
population variance/stdev = n
sample variance/stdev = n-1
7.3